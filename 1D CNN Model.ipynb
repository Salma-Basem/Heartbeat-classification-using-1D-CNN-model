{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2vINYDanLn2",
        "outputId": "6d1dd789-6fe0-43d7-f037-a0c3460770bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (591 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/591.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m591.0/591.0 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (23.1)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow_addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow_addons\n",
            "Successfully installed tensorflow_addons-0.20.0 typeguard-2.13.3\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow_addons"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Dsp-4ZBdWmG",
        "outputId": "673888ac-dd18-4079-da6d-cf20a2ccbf8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MIT Train classes: \n",
            " 0.0    72471\n",
            "4.0     6431\n",
            "2.0     5788\n",
            "1.0     2223\n",
            "3.0      641\n",
            "Name: Class, dtype: int64\n",
            "\n",
            "MIT Test classes: \n",
            " 0.0    18118\n",
            "4.0     1608\n",
            "2.0     1448\n",
            "1.0      556\n",
            "3.0      162\n",
            "Name: Class, dtype: int64\n",
            "(87554, 188)\n",
            "(21892, 188)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
            "\n",
            "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
            "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
            "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
            "\n",
            "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
            "\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Traing dataset size:  (97892, 187, 1)  -- Y size:  (97892, 5)\n",
            "Validation dataset size:  (17511, 187, 1)  -- Y size:  (17511, 5)\n",
            "Test dataset size:  (21892, 187, 1)  -- Y size:  (21892, 5)\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 187, 1)]     0           []                               \n",
            "                                                                                                  \n",
            " conv1d (Conv1D)                (None, 187, 32)      192         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 187, 32)     128         ['conv1d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 187, 32)      5152        ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 187, 32)     128         ['conv1d_1[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " re_lu (ReLU)                   (None, 187, 32)      0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 187, 32)      5152        ['re_lu[0][0]']                  \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 187, 32)     128         ['conv1d_2[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 187, 64)      0           ['batch_normalization[0][0]',    \n",
            "                                                                  'batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " re_lu_1 (ReLU)                 (None, 187, 64)      0           ['concatenate[0][0]']            \n",
            "                                                                                                  \n",
            " max_pooling1d (MaxPooling1D)   (None, 38, 64)       0           ['re_lu_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 38, 32)       10272       ['max_pooling1d[0][0]']          \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 38, 32)      128         ['conv1d_3[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " re_lu_2 (ReLU)                 (None, 38, 32)       0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_4 (Conv1D)              (None, 38, 32)       5152        ['re_lu_2[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 38, 32)      128         ['conv1d_4[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " concatenate_1 (Concatenate)    (None, 38, 96)       0           ['max_pooling1d[0][0]',          \n",
            "                                                                  'batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " re_lu_3 (ReLU)                 (None, 38, 96)       0           ['concatenate_1[0][0]']          \n",
            "                                                                                                  \n",
            " max_pooling1d_1 (MaxPooling1D)  (None, 19, 96)      0           ['re_lu_3[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d_5 (Conv1D)              (None, 19, 32)       15392       ['max_pooling1d_1[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 19, 32)      128         ['conv1d_5[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " re_lu_4 (ReLU)                 (None, 19, 32)       0           ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_6 (Conv1D)              (None, 19, 32)       5152        ['re_lu_4[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 19, 32)      128         ['conv1d_6[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate)    (None, 19, 128)      0           ['max_pooling1d_1[0][0]',        \n",
            "                                                                  'batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " re_lu_5 (ReLU)                 (None, 19, 128)      0           ['concatenate_2[0][0]']          \n",
            "                                                                                                  \n",
            " max_pooling1d_2 (MaxPooling1D)  (None, 4, 128)      0           ['re_lu_5[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d_7 (Conv1D)              (None, 4, 32)        20512       ['max_pooling1d_2[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 4, 32)       128         ['conv1d_7[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " re_lu_6 (ReLU)                 (None, 4, 32)        0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_8 (Conv1D)              (None, 4, 32)        5152        ['re_lu_6[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 4, 32)       128         ['conv1d_8[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate)    (None, 4, 160)       0           ['max_pooling1d_2[0][0]',        \n",
            "                                                                  'batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " re_lu_7 (ReLU)                 (None, 4, 160)       0           ['concatenate_3[0][0]']          \n",
            "                                                                                                  \n",
            " max_pooling1d_3 (MaxPooling1D)  (None, 1, 160)      0           ['re_lu_7[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d_9 (Conv1D)              (None, 1, 32)        25632       ['max_pooling1d_3[0][0]']        \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 1, 32)       128         ['conv1d_9[0][0]']               \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " re_lu_8 (ReLU)                 (None, 1, 32)        0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " conv1d_10 (Conv1D)             (None, 1, 32)        5152        ['re_lu_8[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 1, 32)       128         ['conv1d_10[0][0]']              \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " concatenate_4 (Concatenate)    (None, 1, 192)       0           ['max_pooling1d_3[0][0]',        \n",
            "                                                                  'batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " re_lu_9 (ReLU)                 (None, 1, 192)       0           ['concatenate_4[0][0]']          \n",
            "                                                                                                  \n",
            " max_pooling1d_4 (MaxPooling1D)  (None, 1, 192)      0           ['re_lu_9[0][0]']                \n",
            "                                                                                                  \n",
            " flatten (Flatten)              (None, 192)          0           ['max_pooling1d_4[0][0]']        \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 32)           6176        ['flatten[0][0]']                \n",
            "                                                                                                  \n",
            " dense_1 (Dense)                (None, 5)            165         ['dense[0][0]']                  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 110,661\n",
            "Trainable params: 109,957\n",
            "Non-trainable params: 704\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/50\n",
            "327/327 [==============================] - 144s 418ms/step - loss: 0.3170 - f1_score: 0.9023 - val_loss: 3.6620 - val_f1_score: 0.0922\n",
            "Epoch 2/50\n",
            "327/327 [==============================] - 133s 408ms/step - loss: 0.1040 - f1_score: 0.9654 - val_loss: 0.4710 - val_f1_score: 0.8341\n",
            "Epoch 3/50\n",
            "327/327 [==============================] - 140s 427ms/step - loss: 0.0717 - f1_score: 0.9762 - val_loss: 0.0710 - val_f1_score: 0.9806\n",
            "Epoch 4/50\n",
            "327/327 [==============================] - 136s 415ms/step - loss: 0.0550 - f1_score: 0.9814 - val_loss: 0.3629 - val_f1_score: 0.8622\n",
            "Epoch 5/50\n",
            "327/327 [==============================] - 135s 413ms/step - loss: 0.0426 - f1_score: 0.9856 - val_loss: 0.0768 - val_f1_score: 0.9824\n",
            "Epoch 6/50\n",
            "327/327 [==============================] - 135s 414ms/step - loss: 0.0381 - f1_score: 0.9870 - val_loss: 0.0723 - val_f1_score: 0.9806\n",
            "Epoch 7/50\n",
            "327/327 [==============================] - 134s 409ms/step - loss: 0.0339 - f1_score: 0.9882 - val_loss: 0.0849 - val_f1_score: 0.9753\n",
            "Epoch 8/50\n",
            "327/327 [==============================] - 135s 411ms/step - loss: 0.0250 - f1_score: 0.9920 - val_loss: 0.0735 - val_f1_score: 0.9852\n",
            "Epoch 9/50\n",
            "327/327 [==============================] - 133s 407ms/step - loss: 0.0254 - f1_score: 0.9912 - val_loss: 0.0807 - val_f1_score: 0.9790\n",
            "Epoch 10/50\n",
            "327/327 [==============================] - 135s 413ms/step - loss: 0.0239 - f1_score: 0.9921 - val_loss: 0.0905 - val_f1_score: 0.9748\n",
            "Epoch 11/50\n",
            "327/327 [==============================] - 134s 410ms/step - loss: 0.0210 - f1_score: 0.9930 - val_loss: 0.0649 - val_f1_score: 0.9828\n",
            "Epoch 12/50\n",
            "327/327 [==============================] - 134s 410ms/step - loss: 0.0187 - f1_score: 0.9935 - val_loss: 0.0883 - val_f1_score: 0.9812\n",
            "Epoch 13/50\n",
            "327/327 [==============================] - 132s 402ms/step - loss: 0.0223 - f1_score: 0.9922 - val_loss: 0.0792 - val_f1_score: 0.9830\n",
            "Epoch 14/50\n",
            "327/327 [==============================] - 135s 413ms/step - loss: 0.0172 - f1_score: 0.9943 - val_loss: 0.0732 - val_f1_score: 0.9853\n",
            "Epoch 15/50\n",
            "327/327 [==============================] - 135s 412ms/step - loss: 0.0163 - f1_score: 0.9945 - val_loss: 0.0876 - val_f1_score: 0.9821\n",
            "Epoch 16/50\n",
            "327/327 [==============================] - 134s 409ms/step - loss: 0.0160 - f1_score: 0.9948 - val_loss: 0.1165 - val_f1_score: 0.9833\n",
            "Epoch 17/50\n",
            "327/327 [==============================] - 134s 409ms/step - loss: 0.0140 - f1_score: 0.9954 - val_loss: 0.0724 - val_f1_score: 0.9846\n",
            "Epoch 18/50\n",
            "327/327 [==============================] - 135s 413ms/step - loss: 0.0143 - f1_score: 0.9953 - val_loss: 0.0721 - val_f1_score: 0.9842\n",
            "Epoch 19/50\n",
            "327/327 [==============================] - 134s 409ms/step - loss: 0.0097 - f1_score: 0.9971 - val_loss: 0.1222 - val_f1_score: 0.9681\n",
            "Epoch 20/50\n",
            "327/327 [==============================] - 135s 413ms/step - loss: 0.0119 - f1_score: 0.9959 - val_loss: 0.1226 - val_f1_score: 0.9829\n",
            "Epoch 21/50\n",
            "327/327 [==============================] - 129s 394ms/step - loss: 0.0125 - f1_score: 0.9956 - val_loss: 0.1242 - val_f1_score: 0.9722\n",
            "Epoch 22/50\n",
            "327/327 [==============================] - 127s 389ms/step - loss: 0.0126 - f1_score: 0.9959 - val_loss: 0.0803 - val_f1_score: 0.9850\n",
            "Epoch 23/50\n",
            "327/327 [==============================] - 128s 393ms/step - loss: 0.0082 - f1_score: 0.9973 - val_loss: 0.0779 - val_f1_score: 0.9853\n",
            "Epoch 24/50\n",
            "327/327 [==============================] - 129s 395ms/step - loss: 0.0110 - f1_score: 0.9965 - val_loss: 0.0853 - val_f1_score: 0.9862\n",
            "Epoch 25/50\n",
            "327/327 [==============================] - 126s 386ms/step - loss: 0.0123 - f1_score: 0.9957 - val_loss: 0.0836 - val_f1_score: 0.9835\n",
            "Epoch 26/50\n",
            "327/327 [==============================] - 127s 388ms/step - loss: 0.0099 - f1_score: 0.9968 - val_loss: 0.1740 - val_f1_score: 0.9587\n",
            "Epoch 27/50\n",
            "325/327 [============================>.] - ETA: 0s - loss: 0.0085 - f1_score: 0.9973"
          ]
        }
      ],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "mit_test = pd.read_csv('mitbih_test.csv', header=None)\n",
        "mit_train = pd.read_csv('mitbih_train.csv', header=None)\n",
        "mit_test.rename(columns={187: \"Class\"}, inplace=True)\n",
        "mit_train.rename(columns={187: \"Class\"}, inplace=True)\n",
        "print(\"MIT Train classes: \\n\", mit_train[\"Class\"].value_counts())\n",
        "print(\"\\nMIT Test classes: \\n\", mit_test[\"Class\"].value_counts())\n",
        "print(mit_train.shape)\n",
        "print(mit_test.shape)\n",
        "# Setting Dictionary to define the type of Heartbeat\n",
        "# N: normal\n",
        "# S: Supraventricular premature beat\n",
        "# P: Premature ventricular contraction\n",
        "# F: Fusion of ventricular and normal beat\n",
        "# U: Unclassifiable beat\n",
        "MIT_Outcome = {0.: 'N',\n",
        "               1.: 'S',\n",
        "               2.: 'V',\n",
        "               3.: 'F',\n",
        "               4.: 'Q'}\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.utils import resample\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "from tensorflow import keras\n",
        "from keras.layers import Dense, Conv1D, MaxPool1D, Flatten, Dropout, InputLayer, LSTM, GRU, BatchNormalization, \\\n",
        "    Bidirectional, Concatenate\n",
        "from keras.models import Model\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.utils import to_categorical\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "train_mit, valid_mit = train_test_split(mit_train, test_size=0.20, random_state=42)\n",
        "class_0 = train_mit[train_mit[\"Class\"] == 0]\n",
        "class_1 = train_mit[train_mit[\"Class\"] == 1]\n",
        "class_2 = train_mit[train_mit[\"Class\"] == 2]\n",
        "class_3 = train_mit[train_mit[\"Class\"] == 3]\n",
        "class_4 = train_mit[train_mit[\"Class\"] == 4]\n",
        "# upsampling and equalize the number of sample for each class\n",
        "class_0_upsample = class_0\n",
        "class_1_upsample = resample(class_1, replace=True, n_samples=10000, random_state=123)\n",
        "class_2_upsample = resample(class_2, replace=True, n_samples=10000, random_state=124)\n",
        "class_3_upsample = resample(class_3, replace=True, n_samples=10000, random_state=125)\n",
        "class_4_upsample = resample(class_4, replace=True, n_samples=10000, random_state=126)\n",
        "\n",
        "# combine\n",
        "mit_train = pd.concat([class_0_upsample, class_1_upsample, class_2_upsample, class_3_upsample, class_4_upsample])\n",
        "train_mit, out_train_mit = mit_train.iloc[:, :187], mit_train.iloc[:, -1]\n",
        "valid_mit, out_valid_mit = valid_mit.iloc[:, :187], valid_mit.iloc[:, -1]\n",
        "test_mit, out_test_mit = mit_test.iloc[:, :187], mit_test.iloc[:, -1]\n",
        "# Normalizing the training, validation & test data\n",
        "train_mit = normalize(train_mit, axis=0, norm='max')\n",
        "valid_mit = normalize(valid_mit, axis=0, norm='max')\n",
        "test_mit = normalize(test_mit, axis=0, norm='max')\n",
        "# Reshaping the dataframe into a 3-D Numpy array (batch, Time Period, Value)\n",
        "x_train_mit = train_mit.reshape(len(train_mit), train_mit.shape[1], 1)\n",
        "x_valid_mit = valid_mit.reshape(len(valid_mit), valid_mit.shape[1], 1)\n",
        "x_test_mit = test_mit.reshape(len(test_mit), test_mit.shape[1], 1)\n",
        "\n",
        "# Converting the output into a categorical array\n",
        "y_train_mit = to_categorical(out_train_mit)\n",
        "y_valid_mit = to_categorical(out_valid_mit)\n",
        "y_test_mit = to_categorical(out_test_mit)\n",
        "len(test_mit)\n",
        "print(\"Traing dataset size: \", x_train_mit.shape, \" -- Y size: \", y_train_mit.shape)\n",
        "print(\"Validation dataset size: \", x_valid_mit.shape, \" -- Y size: \", y_valid_mit.shape)\n",
        "print(\"Test dataset size: \", x_test_mit.shape, \" -- Y size: \", y_test_mit.shape)\n",
        "\n",
        "\n",
        "def build_conv1d_model_mit(input_shape=(x_train_mit.shape[1], 1)):\n",
        "    model = keras.models.Sequential()\n",
        "\n",
        "    input_ = tf.keras.layers.Input(shape=(input_shape))\n",
        "\n",
        "    conv1_1 = Conv1D(32, 5, padding='same', input_shape=input_shape)(input_)\n",
        "    conv1_1 = BatchNormalization()(conv1_1)\n",
        "\n",
        "    conv1_2 = Conv1D(32, 5, padding='same')(conv1_1)\n",
        "    conv1_2 = BatchNormalization()(conv1_2)\n",
        "    conv1_2 = tf.keras.layers.ReLU()(conv1_2)\n",
        "\n",
        "    conv1_3 = Conv1D(32, 5, padding='same')(conv1_2)\n",
        "    conv1_3 = BatchNormalization()(conv1_3)\n",
        "\n",
        "    concat_1 = Concatenate()([conv1_1, conv1_3])\n",
        "    concat_1 = tf.keras.layers.ReLU()(concat_1)\n",
        "    max_1 = MaxPool1D(5, padding=\"same\")(concat_1)\n",
        "    #\n",
        "    conv1_4 = Conv1D(32, 5, padding='same')(max_1)\n",
        "    conv1_4 = BatchNormalization()(conv1_4)\n",
        "    conv1_4 = tf.keras.layers.ReLU()(conv1_4)\n",
        "\n",
        "    conv1_5 = Conv1D(32, 5, padding='same')(conv1_4)\n",
        "    conv1_5 = BatchNormalization()(conv1_5)\n",
        "\n",
        "    concat_2 = Concatenate()([max_1, conv1_5])\n",
        "    concat_2 = tf.keras.layers.ReLU()(concat_2)\n",
        "    max_2 = MaxPool1D(5, strides=2, padding=\"same\")(concat_2)\n",
        "    #\n",
        "    conv1_6 = Conv1D(32, 5, padding='same')(max_2)\n",
        "    conv1_6 = BatchNormalization()(conv1_6)\n",
        "    conv1_6 = tf.keras.layers.ReLU()(conv1_6)\n",
        "\n",
        "    conv1_7 = Conv1D(32, 5, padding='same')(conv1_6)\n",
        "    conv1_7 = BatchNormalization()(conv1_7)\n",
        "\n",
        "    concat_3 = Concatenate()([max_2, conv1_7])\n",
        "    concat_3 = tf.keras.layers.ReLU()(concat_3)\n",
        "    max_3 = MaxPool1D(5, padding=\"same\")(concat_3)\n",
        "    #\n",
        "    conv1_8 = Conv1D(32, 5, padding='same')(max_3)\n",
        "    conv1_8 = BatchNormalization()(conv1_8)\n",
        "    conv1_8 = tf.keras.layers.ReLU()(conv1_8)\n",
        "\n",
        "    conv1_9 = Conv1D(32, 5, padding='same')(conv1_8)\n",
        "    conv1_9 = BatchNormalization()(conv1_9)\n",
        "\n",
        "    concat_4 = Concatenate()([max_3, conv1_9])\n",
        "    concat_4 = tf.keras.layers.ReLU()(concat_4)\n",
        "    max_4 = MaxPool1D(5, padding=\"same\")(concat_4)\n",
        "    #\n",
        "    conv1_10 = Conv1D(32, 5, padding='same')(max_4)\n",
        "    conv1_10 = BatchNormalization()(conv1_10)\n",
        "    conv1_10 = tf.keras.layers.ReLU()(conv1_10)\n",
        "\n",
        "    conv1_11 = Conv1D(32, 5, padding='same')(conv1_10)\n",
        "    conv1_11 = BatchNormalization()(conv1_11)\n",
        "\n",
        "    concat_5 = Concatenate()([max_4, conv1_11])\n",
        "    concat_5 = tf.keras.layers.ReLU()(concat_5)\n",
        "    max_5 = MaxPool1D(5, strides=2, padding=\"same\")(concat_5)\n",
        "\n",
        "    flat = Flatten()(max_5)\n",
        "    dense_1 = Dense(32, activation='relu')(flat)\n",
        "    dense_2 = Dense(5, activation=\"softmax\")(dense_1)\n",
        "\n",
        "    model = Model(inputs=input_, outputs=dense_2)\n",
        "\n",
        "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[tfa.metrics.F1Score(2, \"micro\")])\n",
        "    return model\n",
        "\n",
        "\n",
        "checkpoint_cb = ModelCheckpoint(\"senior_multi_diseases_4.h5\", save_best_only=True)\n",
        "\n",
        "model_conv1d_mit = build_conv1d_model_mit(input_shape=(x_train_mit.shape[1], x_train_mit.shape[2]))\n",
        "model_conv1d_mit.summary()\n",
        "history_conv1d_mit = model_conv1d_mit.fit(x_train_mit, y_train_mit, epochs=50, batch_size=300,\n",
        "                                          validation_data=(x_valid_mit, y_valid_mit), callbacks=[checkpoint_cb])\n",
        "model_conv1d_mit.load_weights(\"senior_multi_diseases_4.h5\")\n",
        "model_conv1d_mit.evaluate(x_test_mit, y_test_mit)\n",
        "conv1d_pred_mit_proba = model_conv1d_mit.predict(x_test_mit)\n",
        "# model_conv1d_mit.evaluate(x_train_mit, y_train_mit)\n",
        "# conv1d_pred_mit_proba = model_conv1d_mit.predict (x_train_mit)\n",
        "conv1d_pred_mit = np.argmax(conv1d_pred_mit_proba, axis=1)\n",
        "conv1d_pred_proba_mit = model_conv1d_mit.predict(x_test_mit)\n",
        "# conv1d_pred_proba_mit = model_conv1d_mit.predict (x_train_mit)\n",
        "conv1d_pred_mit = np.argmax(conv1d_pred_proba_mit, axis=1)\n",
        "print(classification_report(out_test_mit, conv1d_pred_mit, target_names=[MIT_Outcome[i] for i in MIT_Outcome]))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}